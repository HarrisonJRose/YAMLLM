# Utility Functions
def strip_all(string):
    """
    Strip all leading and trailing whitespace, and single or double quotes from a string.

    Parameters:
    string (str): The string to be stripped.

    Returns:
    str: The stripped string.
    """
    stripped = string.strip().strip('"\'')
    return stripped

#Main Functions
def tokenize_line(line):
  """
  Process a line of text into a macro token.
  
  This function converts a YAML line into tokens representing its structure, such as list items, keys, and values.
  For example, "- location: clacton-on-sea" becomes a tuple indicating the indentation, token types, and validity.
  
  Parameters:
  line (str): A single line of text from a YAML file.
  
  Returns:
  tuple: A tuple containing the indentation level, a list of tokens, and a boolean indicating validity.
  """
  indent = len(line) - len(line.lstrip())
  tokens = []
  valid = False
  line = line.lstrip()
  
  # Handling list items
  if line.startswith('-'):
      tokens.append(("LIST ITEM",))
      line = line[1:].lstrip()
      indent += 2
      valid = True
  
  # Handling key-value pairs (inc. within lists)
  if ':' in line.split('"')[0]:
      key, value = line.split(':', 1)
      tokens.append(("KEY", key.strip()))
      if value.strip():
          tokens.append(("VALUE", strip_all(value)))
      valid = True
      return (indent, tokens, valid)
  
  # Handling standalone list values
  if valid and len(line.strip()) > 0:
      tokens.append(("VALUE", strip_all(line)))
      return (indent, tokens, valid)
  
  return (indent, tokens, valid)
      
def tokenize_string(string):
  """
  Process a multiline string into micro tokens.

  This function tokenizes an entire YAML string, first into macro tokens and then into micro tokens.

  Parameters:
  string (str): The YAML string to be tokenized.

  Returns:
  list: A list of tokens representing the parsed YAML structure.
  """
  tokens = []
  cur_indent = 0
  for line in string.split('\n'):
      indent, new_tokens, valid = tokenize_line(line)
      if not valid:
          continue
      if indent > cur_indent:
          tokens.append(("INDENT",))
      elif indent < cur_indent:
          tokens.append(('DEINDENT', (cur_indent - indent) // 2))
      cur_indent = indent
      tokens.extend(new_tokens)

  tokens.append(('DEINDENT', cur_indent // 2))
  return tokens


class TokenIterator:
  """
  An iterator for processing a list of tokens.

  This class allows for sequential access to tokens generated by the tokenize_string function, with additional
  utility methods for lookahead and lookback operations.

  Attributes:
  tokens (list): A list of tokens to iterate over.
  """
  def __init__(self, tokens):
      """
      Initializes the TokenIterator with a list of tokens.

      Parameters:
      tokens (list): The tokens to iterate over.
      """
      self.tokens = tokens
      self.position = 0

  def __iter__(self):
      return self

  def __next__(self):
      if self.position < len(self.tokens):
          token = self.tokens[self.position]
          self.position += 1
          return token
      else:
          raise StopIteration

  def lookahead(self, n=1):
      """
      Look ahead 'n' positions from the current token.

      Parameters:
      n (int): Number of positions to look ahead.

      Returns:
      The token 'n' positions ahead of the current position, or None if out of bounds.
      """
      next_pos = self.position + n - 1
      if next_pos < len(self.tokens):
          return self.tokens[next_pos]
      return None

  def lookback(self, n=1):
      """
      Look back 'n' positions from the current token.

      Parameters:
      n (int): Number of positions to look back.

      Returns:
      The token 'n' positions behind the current position, or None if out of bounds.
      """
      prev_pos = self.position - n
      if prev_pos >= 0:
          return self.tokens[prev_pos]
      return None

# Note: The lazy tokenisation is actually slower (~20%) so we don't use it right now. Leaving it in there for future reference.
class LazyTokenIterator:
  """
  Lazily tokenizes a YAML string line by line upon iteration.

  This iterator processes a YAML string incrementally, generating tokens on-demand as the iterator is advanced.
  It's designed to efficiently handle large YAML strings by avoiding upfront tokenization of the entire string.

  Attributes:
  string (str): The YAML string to be tokenized.
  """
  def __init__(self, string):
      """
      Initializes the iterator with a YAML string.

      Parameters:
      string (str): The YAML string to tokenize.
      """
      self.lines = string.split('\n')
      self.num_lines = len(self.lines)
      self.tokens = []
      self.token_position = 0
      self.line_position = 0
      self.cur_indent = 0
      self.verbose = False

  def tokenize_line(self):
      """
      Tokenizes the current line and updates the token list and current indentation.

      This method processes the current line to generate tokens, adjusting the current indentation
      level as necessary. It advances the line position after processing.
      """
      line = self.lines[self.line_position]
      indent, new_tokens, valid = tokenize_line(line)

      if not valid:
          self.line_position += 1
          return

      if indent > self.cur_indent:
          self.tokens.append(("INDENT",))
      elif indent < self.cur_indent:
          self.tokens.append(('DEINDENT', (self.cur_indent - indent) // 2))
      self.cur_indent = indent
      self.tokens.extend(new_tokens)

      if self.line_position == self.num_lines:
          self.tokens.append(('DEINDENT', self.cur_indent // 2))

      self.line_position += 1

  def __iter__(self):
      return self

  def __next__(self):
      """
      Returns the next token in the sequence or tokenizes the next line if needed.

      This method attempts to return the next available token. If the current tokens are exhausted
      but lines remain, it tokenizes the next line to generate more tokens.

      Returns:
      tuple: The next token in the sequence.

      Raises:
      StopIteration: If there are no more tokens and no more lines to tokenize.
      """
      if self.token_position < len(self.tokens):
          token = self.tokens[self.token_position]
          self.token_position += 1
          return token
      elif self.line_position < self.num_lines:
          self.tokenize_line()
          return self.__next__()
      else:
          raise StopIteration

  def lookahead(self, n=1):
      """
      Looks ahead 'n' tokens in the sequence, tokenizing more lines if necessary.

      Parameters:
      n (int): The number of tokens to look ahead.

      Returns:
      tuple or None: The 'n'th token ahead of the current one, or None if out of range.
      """
      next_pos = self.token_position + n - 1

      if next_pos < len(self.tokens):
          return self.tokens[next_pos]
      elif self.line_position < self.num_lines:
          self.tokenize_line()
          return self.lookahead(n)
      return None

  def lookback(self, n=1):
      """
      Looks back 'n' tokens in the sequence.

      Parameters:
      n (int): The number of tokens to look back.

      Returns:
      tuple or None: The 'n'th token behind the current one, or None if out of range.
      """
      prev_pos = self.token_position - n
      if prev_pos >= 0:
          return self.tokens[prev_pos]
      return None

      
def parse_tokens(tokens):
  """
  Parses a list of tokens into a hierarchical Python object.

  This function interprets a series of tokens representing YAML structure, such as indentation,
  keys, values, and list items, converting them into a nested Python dictionary or list structure.

  Parameters:
  tokens (list): A list of tokens generated by `tokenize_string`.

  Returns:
  dict or list: The Python object representing the parsed YAML content.
  """
  result = {}
  stack = [result]
  cur_indent = 0
  token_iter = TokenIterator(tokens)
  for token in token_iter:

    #SIMPLE TOKENS
    #----------------------------------

    #HANDLE DEINDENT, CLOST LAST ITEM ON STACK
    if token[0] == 'DEINDENT':
      levels = token[1]
      for _ in range(levels):
        cur_indent -= 1
        stack.pop()
      continue

    #DOUBLE TOKENS
    #----------------------------------
    next_token = token_iter.lookahead()
    if next_token == None:
      break
      
    #HANDLE CLASSIC KEY VALUE PAIRS
    if token[0] == 'KEY' and next_token[0] == 'VALUE':
      key = token[1]
      value = next_token[1]
      stack[-1][key] = value
      next(token_iter)
      continue

    #HANDLE SIMPLE LIST ITEMS
    if token[0] == 'LIST ITEM' and next_token[0] == 'VALUE':
      value = next_token[1]
      stack[-1].append(value)
      next(token_iter)
      continue

    #HANDLE LISTS CONTAINING KEYS
    if token[0] == 'LIST ITEM' and next_token[0] == 'KEY':
      #If we're in a dict, close it
      if isinstance(stack[-1], dict):
        stack.pop()
      
      new_dict = {}
      stack[-1].append(new_dict)
      stack.append(new_dict)
      continue

    #TRIPLE TOKENS
    #----------------------------------
    next_next_token = token_iter.lookahead(2)
    if next_next_token == None:
      break

    #HANDLE KEYS CONTAINING KEYS
    if token[0] == 'KEY' and next_token[0] == 'INDENT' and next_next_token[0] == 'KEY':
      cur_indent += 1
      key = token[1]
      new_dict = {}
      stack[-1][key] = new_dict
      stack.append(new_dict)
      next(token_iter) #Skip the indent, don't consume child key
      continue

    #HANDLE KEYS CONTAINING LISTS
    if token[0] == 'KEY' and next_token[0] == 'INDENT' and next_next_token[0] == 'LIST ITEM':
      cur_indent += 1
      key = token[1]
      new_list = []
      stack[-1][key] = new_list
      stack.append(new_list)
      next(token_iter) #Skip the indent, don't consume LIST ITEM
      continue

  return result


class atr:
  """
  Attribute class for use in AYAML schemas.

  This class provides a way to define attributes in AYAML schemas. By default we always have type, and description.

  Attributes:
  type (str): The type of the attribute.
  description (str): A description of the attribute.
  required (bool): Whether the attribute is required.
  validate_func(function): A function to validate the attribute value.
  fix_func(function): A function to fix the attribute value.
  """
  def __init__(self, type_, description, required=False, always_cast=True, validate_func=None, fix_func=None):
      self.type = type_
      self.description = description
      self.required = required
      self.validate_func = validate_func
      self.fix_func = fix_func
      self.always_cast = always_cast

  def validate(self, value):
      """
      Validates the value against schema type, and optionally fixer and validation functions.
      
      Parameters:
      value (any): The value to validate.
      Returns:
      bool: True if the value is valid, False otherwise.
      value (any): Cast and corrected value if True, otherwise error message"""
      # Attempt to correct the value if a fix function is provided
      if self.fix_func is not None:
          value = self.fix_func(value)

      # Check type. As LLMs output strings we almost always cast to the type
      if self.always_cast:
          try:
              value = self.type(value)
          except ValueError:
              return False, f"Could not cast {value} into type '{self.type}'"

      # Run custom validation function if provided
      if self.validate_func is not None and not self.validate_func(value):
          return False, "Failed custom validation"

      return True, value


def validate_against_schema(object, schema, route=""):
  """
  Validates an object against a schema.
  
  Parameters:
  object (dict or list): The object to validate.
  schema (dict): The schema to validate against.
  route (str): The current route in the schema. (recursive)
  
  Returns:
  bool: Could the object be validated
  object or None: The object cast to its schema or None for invalid objects"""
  
  
  if isinstance(object, dict):
    for key, value in object.items():
      if key not in schema:
        print(f"Invalid key: {key}. Expected keys: {schema.keys()} at {route}")
        return False, None
      success, new_child = validate_against_schema(value, schema[key], route + f".{key}")
      if success:
        object[key] = new_child
      else:
        return False, None

  #If not list or dict, we're assuming it's a value, hence our schema should have a tuple
  elif isinstance(schema, atr):
    #Attempt to cast our value to expected type
    success, value = schema.validate(object)
    if not success:
      print(value)
      print(f"At: {route}")
      return False, None
    return True, value
  
  elif isinstance(object, list):
    #Check our schema is a list
    if not isinstance(schema, list):
      print(f"Invalid schema. Expected a list at {route}, instead got {type(schema)} with value: {schema}")
      return False, None
    for count, item in enumerate(object):
      success, new_child = validate_against_schema(item, schema[0], route + f"[{count}]")
      if success:
        object[count] = new_child
      else:
        return False, None
  
  return True, object


def print_schema(schema, indent=0):
  """
  Prints a schema in a human-readable YAML-style format for use in LLM prompts

  Parameters:
  schema (any): The (partial) schema to print.
  indent (int): The current indentation level.
  Returns:
  str: The schema in a human-readable format.
  """
  schema_string = ""
  indent_string = "  " * indent

  # If dict, write the key and go deeper
  if isinstance(schema, dict):
    for key, value in schema.items():
      schema_string += f"\n{indent_string}{key}:"
      schema_string += print_schema(value, indent + 1)

  # If atr, print the type and description
  elif isinstance(schema, atr):
    type_string = schema.type.__name__
    description_string = schema.description
    schema_string += f" # {type_string} {description_string}"    

  # If list, iterate through the items and go deeper
  elif isinstance(schema, list):
    for item in schema:
      schema_string += f"\n{indent_string}- "
      schema_string += print_schema(
          item, indent + 1).lstrip()  #We wamt the first item inline with out -
    
  else:
    schema_string += f"{indent_string}{schema}"

  return schema_string

def parse_and_validate(string, schema):
  """
  Parses a string and validates it against a schema.
  Parameters:
  string (str): The string to parse and validate.
  schema (dict): The schema to validate against.
  Returns:
  dict: The parsed and validated object.
  """
  tokens = tokenize_string(string)
  parsed_object = parse_tokens(tokens)
  success, validated_object = validate_against_schema(parsed_object, schema)
  if success:
    return validated_object
  else:
    return None